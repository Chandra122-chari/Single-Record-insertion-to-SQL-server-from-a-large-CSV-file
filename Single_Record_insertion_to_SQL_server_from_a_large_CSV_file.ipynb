{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ac37f-0fd0-46c9-a465-3ec195620600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server\n",
      "Table 'Airline_data' created successfully with inferred schema.\n",
      "Inserting rows...........................\n",
      "Inserted row 200\n",
      "Inserted row 201\n",
      "Inserted row 202\n",
      "Inserted row 203\n",
      "Inserted row 204\n",
      "Inserted row 205\n",
      "Inserted row 206\n",
      "Inserted row 207\n",
      "Inserted row 208\n",
      "Inserted row 209\n",
      "Schedule stopped.\n",
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a database connection\n",
    "def create_connection(server, database):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=\" + server + \";DATABASE=\" + database + \";Trusted_Connection=yes\")\n",
    "        print(\"Connected to SQL Server\")\n",
    "    except pyodbc.Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "# Function to create a dataframe out of csv file\n",
    "def create_dataframe(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to create table in SQL server from dataframe\n",
    "def create_table_with_inferred_schema(conn, data, table_name, pk_dtype, primary_key=None):\n",
    "    columns = list(data.columns)\n",
    "    cursor = conn.cursor()\n",
    "    # Dictionary to map pandas data types to SQL Server data types\n",
    "    type_mapping = {\n",
    "        'object': 'VARCHAR(MAX)',\n",
    "        'int64': 'INT',\n",
    "        'float64': 'FLOAT',\n",
    "        'datetime64': 'DATETIME'\n",
    "        # Add more mappings as needed for other data types\n",
    "    }\n",
    "    # Infer data types for each column\n",
    "    column_types = [type_mapping[str(data[col].dtype)] for col in columns]\n",
    "    # Override the data type for the primary key column if specified\n",
    "    if primary_key:\n",
    "        column_types[columns.index(primary_key)] = pk_dtype  # Change the data type to INT for example\n",
    "    # Construct the SQL query to create the table with inferred schema\n",
    "    query = f\"IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_name}')\"\n",
    "    query += f\" CREATE TABLE {table_name} ({', '.join([f'[{col}] {data_type}' for col, data_type in zip(columns, column_types)])}\"\n",
    "    if primary_key:\n",
    "        query += f\", PRIMARY KEY ({primary_key})\"\n",
    "    query += \")\"\n",
    "    try:\n",
    "        # Execute the SQL query\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        print(f\"Table '{table_name}' created successfully with inferred schema.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "def insert_row_into_database(dataframe, table_name, conn):\n",
    "    cursor = conn.cursor()\n",
    "    # Get the primary key column name from the database\n",
    "    cursor.execute(f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE WHERE OBJECTPROPERTY(OBJECT_ID(CONSTRAINT_SCHEMA + '.' + QUOTENAME(CONSTRAINT_NAME)), 'IsPrimaryKey') = 1 AND TABLE_NAME = '{table_name}'\")\n",
    "    primary_key_column = cursor.fetchone()[0]\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in dataframe.iterrows():\n",
    "        try:\n",
    "            # Check if the row already exists in the database using the primary key column\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name} WHERE {primary_key_column} = ?\", (row[primary_key_column],))\n",
    "                \n",
    "            if cursor.fetchone()[0] == 0:\n",
    "                # If the row doesn't exist, insert it into the database\n",
    "                columns = ', '.join(f\"[{col}]\" for col in dataframe.columns)\n",
    "                placeholders = ', '.join(['?' for _ in range(len(dataframe.columns))])\n",
    "                sql_query = f'''\n",
    "                    INSERT INTO {table_name} ({columns}) \n",
    "                    VALUES ({placeholders})\n",
    "                '''\n",
    "                values = tuple(row[col] for col in dataframe.columns)\n",
    "                cursor.execute(sql_query, values)\n",
    "                print(f\"Inserted row {index}\")\n",
    "                conn.commit()  # Commit the changes after each successful insertion\n",
    "                break\n",
    "            #else:\n",
    "                #print(f\"Row {index} already exists in the database\")\n",
    "                    \n",
    "        except (pyodbc.Error, ValueError) as e:\n",
    "            print(f\"Error inserting row {index}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    server = \"your server name\"\n",
    "    database = \"database name\"\n",
    "    table_name = \"Airline_data\"\n",
    "    file_path = 'C:/Users\\chand\\OneDrive\\Documents\\Airline_Dataset.csv'\n",
    "    pk_dtype = 'VARCHAR(20)'\n",
    "    primary_key = 'Passenger_ID'\n",
    "    \n",
    "    conn = create_connection(server, database)\n",
    "    data = create_dataframe(file_path)\n",
    "    create_table_with_inferred_schema(conn, data, table_name, pk_dtype, primary_key)\n",
    "    dataframe = data\n",
    "    \n",
    "    # Counter to track the number of iterations\n",
    "    global iteration_count\n",
    "    iteration_count = 0\n",
    "    def my_function():\n",
    "        global iteration_count\n",
    "        # Call the insert_row_into_database function\n",
    "        insert_row_into_database(dataframe, table_name, conn)\n",
    "        iteration_count += 1\n",
    "        if iteration_count >= 10:  # Stop after given number of iterations\n",
    "            schedule.clear()  # Clear all scheduled jobs\n",
    "            print(\"Schedule stopped.\")\n",
    "            %killbgscripts  # Terminate any background scripts\n",
    "    print(\"Inserting rows...........................\")\n",
    "    # Schedule the function to run every 1 minutes\n",
    "    schedule.every(1).seconds.do(my_function)\n",
    "\n",
    "    # Run the scheduler loop\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)  # Sleep for 1 second to avoid high CPU usage\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796179c-7cc0-45ad-afcd-cebe9ed89190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
